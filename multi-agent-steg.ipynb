{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from trl import PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "import wandb\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "class StegEnv():\n",
    "    def __init__(self, \n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 batch_size: int = 16,\n",
    "                 ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.key_length = 4 # must be >= 4 for now because of ppo_train\n",
    "\n",
    "        self.prompts = [\" 0 7 3 8\", \"This morning I went to the \", \"The weather today is \", \"What is your favorite \"]\n",
    "        self.key_tokens = [' 0', ' 1', ' 2', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 9']\n",
    "\n",
    "        self.enc_response_len = 4\n",
    "        self.dec_response_len = self.key_length\n",
    "\n",
    "        self.prompt_batch = None\n",
    "        self.key_batch = None\n",
    "\n",
    "\n",
    "    def _get_queries(self):\n",
    "        return [\"key:\" + key + \" prompt:\" + prompt for key, prompt in zip(self.key_batch, self.prompt_batch)]\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        queries = self._get_queries()\n",
    "        query_pt = [self.tokenizer.encode(query, return_tensors=\"pt\").squeeze() for query in queries]\n",
    "        prompt_pt = [self.tokenizer.encode(prompt, return_tensors=\"pt\").squeeze() for prompt in self.prompt_batch]\n",
    "        return {\n",
    "            \"query\": queries,\n",
    "            \"query_pt\": query_pt,\n",
    "            \"prompt\": self.prompt_batch,\n",
    "            \"prompt_pt\": prompt_pt,\n",
    "            \"key\": self.key_batch\n",
    "        }\n",
    "\n",
    "    def reset(self, ):\n",
    "        prompt_idxs = torch.randint(len(self.prompts), size=(self.batch_size,))\n",
    "        key_idxs = torch.randint(len(self.key_tokens), size=(self.batch_size, self.key_length))\n",
    "\n",
    "        self.prompt_batch = [self.prompts[idx] for idx in prompt_idxs]\n",
    "        self.key_batch = [\"\".join([self.key_tokens[idx] for idx in idxs]) for idxs in key_idxs]\n",
    "\n",
    "        print(self.prompt_batch)\n",
    "        print(self.key_batch)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _reward_function(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        rewards_encoder = []\n",
    "        rewards_decoder = []\n",
    "\n",
    "        for response_enc, response_dec, key in zip(responses_encoder, responses_decoder, self.key_batch):\n",
    "            key_pt = self.tokenizer.encode(key, return_tensors=\"pt\").squeeze()\n",
    "\n",
    "            # if response_dec_txt == key:\n",
    "            #     reward_enc, reward_dec = 1.0, 1.0\n",
    "            # else:\n",
    "            #     reward_enc, reward_dec = 0.0, 0.0\n",
    "\n",
    "            reward_enc = reward_dec = (key_pt == response_dec).sum().float()\n",
    "\n",
    "            rewards_encoder.append(reward_enc)\n",
    "            rewards_decoder.append(reward_dec)\n",
    "\n",
    "        return (rewards_encoder, rewards_decoder)\n",
    "\n",
    "    def step(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        reward = self._reward_function(responses_encoder, responses_decoder)\n",
    "        return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StegPPOTrainer():\n",
    "    def __init__(self,\n",
    "            model: AutoModel,\n",
    "            model_ref: AutoModel,\n",
    "            tokenizer: AutoTokenizer,\n",
    "            multi_agent = True,\n",
    "\n",
    "        ):\n",
    "        \n",
    "        self.multi_agent = multi_agent\n",
    "        batch_size = 16\n",
    "        self.num_episodes = 100\n",
    "\n",
    "        self.model = model\n",
    "        self.model_ref = model_ref\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # initialize environment\n",
    "        self.env = StegEnv(\n",
    "            tokenizer = self.tokenizer,\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "        # get tokens to suppress\n",
    "        key_token_ids = self.tokenizer(self.env.key_tokens)['input_ids']\n",
    "        self.suppress_tokens = [i for i in range(self.tokenizer.vocab_size) if [i] not in key_token_ids]\n",
    "\n",
    "        config = PPOConfig(\n",
    "            batch_size= batch_size * 2 if multi_agent else batch_size, # double for encoder + decoder responses\n",
    "            learning_rate=1e-5,\n",
    "            steps=5000,\n",
    "            )\n",
    "            \n",
    "        self.ppo_trainer = PPOTrainer(config, self.model, self.model_ref, self.tokenizer)\n",
    "        \n",
    "    def log_stats(\n",
    "        self,\n",
    "        stats: dict,\n",
    "        batch: dict,\n",
    "        rewards: List[torch.FloatTensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function that logs all the training stats. Call it at the end of each epoch.\n",
    "\n",
    "        Args:\n",
    "            stats (dict[str, Any]):\n",
    "                A dictionary of training stats.\n",
    "            batch (dict[str, Any]):\n",
    "                A dictionary of batch data, this contains the queries and responses.\n",
    "            rewards (`List[torch.FloatTensor]`):\n",
    "                A tensor of rewards.\n",
    "        \"\"\"\n",
    "        logs = {}\n",
    "\n",
    "        # Log stats\n",
    "        if not isinstance(rewards, torch.Tensor):\n",
    "            rewards = torch.tensor(rewards) #.to(self.current_device)\n",
    "\n",
    "        logs.update(stats)\n",
    "\n",
    "        # manually cast in fp32 for bf16 torch tensors\n",
    "        for k, v in logs.items():\n",
    "            if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:\n",
    "                logs[k] = v.float()\n",
    "\n",
    "        logs[\"env/reward_mean\"] = torch.mean(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_dist\"] = rewards.cpu().numpy()\n",
    "\n",
    "        wandb.log(logs)\n",
    "\n",
    "    def get_generation_kwargs(self):\n",
    "\n",
    "        encoder_generation_kwargs = {\n",
    "            \"min_length\": -1,\n",
    "            \"top_k\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"max_new_tokens\": self.env.enc_response_len\n",
    "        }\n",
    "\n",
    "        decoder_generation_kwargs = {\"suppress_tokens\": self.suppress_tokens, \n",
    "                                    \"max_new_tokens\": self.env.dec_response_len, \n",
    "                                    **encoder_generation_kwargs}\n",
    "        \n",
    "        return encoder_generation_kwargs, decoder_generation_kwargs\n",
    "\n",
    "    def get_model_responses(self, obs):\n",
    "        \n",
    "        encoder_queries = []\n",
    "        decoder_queries = []\n",
    "        encoder_responses = []\n",
    "        decoder_responses = []\n",
    "\n",
    "        enc_gen_kwargs, dec_gen_kwargs = self.get_generation_kwargs()\n",
    "\n",
    "        for query_encoder_pt, prompt_encoder_pt in zip(obs['query_pt'], obs['prompt_pt']):\n",
    "\n",
    "            # get encoder response on prompt\n",
    "            response_encoder_pt = self.ppo_trainer.generate(query_encoder_pt, **enc_gen_kwargs)\n",
    "            response_encoder_pt = response_encoder_pt.squeeze()[-enc_gen_kwargs[\"max_new_tokens\"]:]\n",
    "            encoder_queries.append(query_encoder_pt)\n",
    "            encoder_responses.append(response_encoder_pt)\n",
    "\n",
    "            # construct decoder query which exclues key\n",
    "            query_decoder_pt = torch.cat((prompt_encoder_pt, response_encoder_pt))\n",
    "\n",
    "            # get decoder response\n",
    "            response_decoder_pt = self.ppo_trainer.generate(query_decoder_pt, **dec_gen_kwargs)\n",
    "            response_decoder_pt = response_decoder_pt.squeeze()[-dec_gen_kwargs[\"max_new_tokens\"]:]\n",
    "            \n",
    "            decoder_queries.append(query_decoder_pt)\n",
    "            decoder_responses.append(response_decoder_pt)\n",
    "\n",
    "        return encoder_queries, encoder_responses, decoder_queries, decoder_responses\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for _ in range(self.num_episodes):\n",
    "            obs = self.env.reset()\n",
    "            encoder_queries, encoder_responses, decoder_queries, decoder_responses = self.get_model_responses(obs)\n",
    "            encoder_rewards, decoder_rewards = self.env.step(encoder_responses, decoder_responses)\n",
    "\n",
    "            print(obs['query'])\n",
    "            print('Encoder')\n",
    "            print('respsonses: ', [self.tokenizer.decode(res) for res in encoder_responses])\n",
    "            print('rewards: ', encoder_rewards)\n",
    "            print()\n",
    "            print('Deconder')\n",
    "            print('respsonses: ', [self.tokenizer.decode(res) for res in decoder_responses])\n",
    "            print('rewards: ', decoder_rewards)\n",
    "            print()\n",
    "            print('-----------------------------------------------------------------------')\n",
    "\n",
    "            if self.multi_agent:\n",
    "\n",
    "                queries = encoder_queries + decoder_queries\n",
    "                responses = encoder_responses + decoder_responses\n",
    "                rewards = encoder_rewards + decoder_rewards\n",
    "\n",
    "                stats = self.ppo_trainer.step(queries, responses, rewards)\n",
    "\n",
    "            else:\n",
    "                stats = self.ppo_trainer.step(encoder_queries, encoder_responses, encoder_rewards,\n",
    "                                      decoder_queries, decoder_responses, decoder_rewards)\n",
    "\n",
    "            # ppo_encoder_trainer.log_stats(stats1, {'query':encoder_queries, 'response': encoder_responses}, encoder_rewards)\n",
    "            self.log_stats(stats, {'query':decoder_queries, 'response': decoder_responses}, decoder_rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kwzd2g4h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2135, in _atexit_cleanup\n",
      "    self._on_finish()\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2372, in _on_finish\n",
      "    _ = exit_handle.wait(timeout=-1, on_progress=self._on_progress_exit)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 298, in wait\n",
      "    on_probe(probe_handle)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2337, in _on_probe_exit\n",
      "    result = handle.wait(timeout=0)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 281, in wait\n",
      "    raise MailboxError(\"transport failed\")\n",
      "wandb.sdk.lib.mailbox.MailboxError: transport failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1145, in init\n",
      "    run = wi.init()\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 580, in init\n",
      "    latest_run.finish()\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 393, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 334, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1883, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1898, in _finish\n",
      "    self._atexit_cleanup(exit_code=exit_code)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2146, in _atexit_cleanup\n",
      "    self._backend.cleanup()\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\backend\\backend.py\", line 255, in cleanup\n",
      "    self.interface.join()\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 638, in join\n",
      "    super().join()\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 741, in join\n",
      "    _ = self._communicate_shutdown()\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 551, in _communicate_shutdown\n",
      "    _ = self._communicate(record)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 285, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 60, in _communicate_async\n",
      "    future = self._router.send_and_receive(rec, local=local)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\interface\\router.py\", line 94, in send_and_receive\n",
      "    self._send_message(rec)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\interface\\router_sock.py\", line 36, in _send_message\n",
      "    self._sock_client.send_record_communicate(record)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 216, in send_record_communicate\n",
      "    self.send_server_request(server_req)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "multi_agent = True\n",
    "\n",
    "if multi_agent:\n",
    "    from trl import PPOTrainer\n",
    "else:\n",
    "    from trl_custom import PPOTrainer\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2').to(device)\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2').to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "wandb.init(project=\"my-awesome-project\")\n",
    "\n",
    "steg_trainer = StegPPOTrainer(model, model_ref, tokenizer, multi_agent=multi_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 0 7 3 8', 'What is your favorite ', 'This morning I went to the ', ' 0 7 3 8', 'What is your favorite ', 'What is your favorite ', 'What is your favorite ', 'What is your favorite ', 'This morning I went to the ', 'What is your favorite ', 'This morning I went to the ', 'The weather today is ', ' 0 7 3 8', 'What is your favorite ', 'The weather today is ', ' 0 7 3 8']\n",
      "[' 6 8 4 3', ' 6 9 1 4', ' 4 1 9 9', ' 9 0 1 2', ' 3 0 5 5', ' 2 9 1 8', ' 8 3 6 9', ' 1 7 3 5', ' 2 1 0 9', ' 3 1 1 0', ' 3 6 6 7', ' 9 6 3 4', ' 5 0 8 2', ' 8 2 7 5', ' 0 0 8 1', ' 9 6 1 0']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m steg_trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[2], line 122\u001b[0m, in \u001b[0;36mStegPPOTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_episodes):\n\u001b[0;32m    121\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 122\u001b[0m     encoder_queries, encoder_responses, decoder_queries, decoder_responses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_model_responses(obs)\n\u001b[0;32m    123\u001b[0m     encoder_rewards, decoder_rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(encoder_responses, decoder_responses)\n\u001b[0;32m    125\u001b[0m     \u001b[39mprint\u001b[39m(obs[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[2], line 101\u001b[0m, in \u001b[0;36mStegPPOTrainer.get_model_responses\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     96\u001b[0m enc_gen_kwargs, dec_gen_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_generation_kwargs()\n\u001b[0;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m query_encoder_pt, prompt_encoder_pt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(obs[\u001b[39m'\u001b[39m\u001b[39mquery_pt\u001b[39m\u001b[39m'\u001b[39m], obs[\u001b[39m'\u001b[39m\u001b[39mprompt_pt\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m     \u001b[39m# get encoder response on prompt\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     response_encoder_pt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mppo_trainer\u001b[39m.\u001b[39mgenerate(query_encoder_pt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39menc_gen_kwargs)\n\u001b[0;32m    102\u001b[0m     response_encoder_pt \u001b[39m=\u001b[39m response_encoder_pt\u001b[39m.\u001b[39msqueeze()[\u001b[39m-\u001b[39menc_gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_new_tokens\u001b[39m\u001b[39m\"\u001b[39m]:]\n\u001b[0;32m    103\u001b[0m     encoder_queries\u001b[39m.\u001b[39mappend(query_encoder_pt)\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:375\u001b[0m, in \u001b[0;36mPPOTrainer.generate\u001b[1;34m(self, query_tensor, **generation_kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, query_tensor: torch\u001b[39m.\u001b[39mTensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgeneration_kwargs):\n\u001b[0;32m    361\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39m    Generate response with the model given the query tensor.\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39m    call the `generate` method of the model.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39m        `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39munwrap_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m--> 375\u001b[0m         input_ids\u001b[39m=\u001b[39mquery_tensor\u001b[39m.\u001b[39;49munsqueeze(dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgeneration_kwargs\n\u001b[0;32m    376\u001b[0m     )\n\u001b[0;32m    378\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steg_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
