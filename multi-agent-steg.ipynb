{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "import wandb\n",
    "from typing import Dict, Tuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StegEnv():\n",
    "    def __init__(self, \n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 batch_size: int = 16,\n",
    "                 ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size # must be the same as value set in ppo_config\n",
    "\n",
    "        self.prompts = [\" 0 3 4\", \"This morning I went to the \", \"The weather today is \", \"What is your favorite \"]\n",
    "        self.key_tokens = [\" 0\", \" 1\", \" 2\", \" 4\"]\n",
    "\n",
    "        self.enc_response_len = 4\n",
    "        self.dec_response_len = 4\n",
    "\n",
    "        self.prompt_batch = None\n",
    "        self.secret_batch = None\n",
    "\n",
    "\n",
    "    def _get_queries(self):\n",
    "        return [\"secret:\" + secret + \" prompt:\" + prompt for secret, prompt in zip(self.secret_batch, self.prompt_batch)]\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        queries = self._get_queries()\n",
    "        query_pt = [self.tokenizer.encode(query, return_tensors=\"pt\").squeeze() for query in queries]\n",
    "        prompt_pt = [self.tokenizer.encode(prompt, return_tensors=\"pt\").squeeze() for prompt in self.prompt_batch]\n",
    "        return {\n",
    "            \"query\": queries,\n",
    "            \"query_pt\": query_pt,\n",
    "            \"prompt\": self.prompt_batch,\n",
    "            \"prompt_pt\": prompt_pt,\n",
    "            \"secret\": self.secret_batch\n",
    "        }\n",
    "\n",
    "    def reset(self, ):\n",
    "        prompt_idxs = torch.randint(len(self.prompts), size=(self.batch_size,))\n",
    "        secret_idxs = torch.randint(len(self.key_tokens), size=(self.batch_size,))\n",
    "\n",
    "        self.prompt_batch = [self.prompts[idx] for idx in prompt_idxs]\n",
    "        self.secret_batch = [self.key_tokens[idx] for idx in secret_idxs]\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _reward_function(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        rewards_encoder = []\n",
    "        rewards_decoder = []\n",
    "\n",
    "        for response_enc, response_dec, secret in zip(responses_encoder, responses_decoder, self.secret_batch):\n",
    "            secret_pt = self.tokenizer.encode(secret, return_tensors=\"pt\").squeeze()\n",
    "\n",
    "            # if response_dec_txt == secret:\n",
    "            #     reward_enc, reward_dec = 1.0, 1.0\n",
    "            # else:\n",
    "            #     reward_enc, reward_dec = 0.0, 0.0\n",
    "\n",
    "            reward_enc = reward_dec = (secret_pt == response_dec).sum().float()\n",
    "\n",
    "            rewards_encoder.append(reward_enc)\n",
    "            rewards_decoder.append(reward_dec)\n",
    "\n",
    "        return (rewards_encoder, rewards_decoder)\n",
    "\n",
    "    def step(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        reward = self._reward_function(responses_encoder, responses_decoder)\n",
    "        return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StegPPOTrainer():\n",
    "    def __init__(self,\n",
    "            multi_agent = True,\n",
    "        ):\n",
    "        \n",
    "        self.multi_agent = multi_agent\n",
    "        batch_size = 16\n",
    "        self.num_episodes = 100\n",
    "\n",
    "        # 1. load a pretrained model\n",
    "        self.model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "        self.model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # get tokens to suppress\n",
    "        key_token_ids = self.tokenizer(self.env.key_tokens)['input_ids']\n",
    "        self.suppress_tokens = [i for i in range(self.tokenizer.vocab_size) if [i] not in key_token_ids]\n",
    "\n",
    "        # 2. initialize trainer\n",
    "        config = PPOConfig(\n",
    "            batch_size=batch_size * 2, # double for encoder + decoder responses\n",
    "            learning_rate=1e-5,\n",
    "            steps=5000,\n",
    "            )\n",
    "        self.ppo_trainer = PPOTrainer(config, self.model, self.model_ref, self.tokenizer)\n",
    "\n",
    "        # initialize environment\n",
    "        self.env = StegEnv(\n",
    "            tokenizer = self.tokenizer,\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "        wandb.init(\n",
    "            # Set the project where this run will be logged\n",
    "            project=\"my-awesome-project\",\n",
    "            # Track hyperparameters and run metadata\n",
    "            )\n",
    "        \n",
    "    def log_stats(\n",
    "        self,\n",
    "        stats: dict,\n",
    "        batch: dict,\n",
    "        rewards: List[torch.FloatTensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function that logs all the training stats. Call it at the end of each epoch.\n",
    "\n",
    "        Args:\n",
    "            stats (dict[str, Any]):\n",
    "                A dictionary of training stats.\n",
    "            batch (dict[str, Any]):\n",
    "                A dictionary of batch data, this contains the queries and responses.\n",
    "            rewards (`List[torch.FloatTensor]`):\n",
    "                A tensor of rewards.\n",
    "        \"\"\"\n",
    "        logs = {}\n",
    "\n",
    "        # Log stats\n",
    "        if not isinstance(rewards, torch.Tensor):\n",
    "            rewards = torch.tensor(rewards) #.to(self.current_device)\n",
    "\n",
    "        logs.update(stats)\n",
    "\n",
    "        # manually cast in fp32 for bf16 torch tensors\n",
    "        for k, v in logs.items():\n",
    "            if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:\n",
    "                logs[k] = v.float()\n",
    "\n",
    "        logs[\"env/reward_mean\"] = torch.mean(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_dist\"] = rewards.cpu().numpy()\n",
    "\n",
    "        wandb.log(logs)\n",
    "\n",
    "    def get_generation_kwargs(self):\n",
    "\n",
    "        encoder_generation_kwargs = {\n",
    "            \"min_length\": -1,\n",
    "            \"top_k\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"max_new_tokens\": self.env.enc_response_len\n",
    "        }\n",
    "\n",
    "        decoder_generation_kwargs = {\"suppress_tokens\": self.suppress_tokens, \n",
    "                                    \"max_new_tokens\": self.env.dec_response_len, \n",
    "                                    **encoder_generation_kwargs}\n",
    "        \n",
    "        return encoder_generation_kwargs, decoder_generation_kwargs\n",
    "\n",
    "    def get_model_responses(self, obs):\n",
    "        \n",
    "        encoder_queries = []\n",
    "        decoder_queries = []\n",
    "        encoder_responses = []\n",
    "        decoder_responses = []\n",
    "\n",
    "        enc_gen_kwargs, dec_gen_kwargs = self.get_generation_kwargs()\n",
    "\n",
    "        for query_encoder_pt, prompt_encoder_pt in zip(obs['query_pt'], obs['prompt_pt']):\n",
    "\n",
    "            # get encoder response on prompt\n",
    "            response_encoder_pt = self.ppo_trainer.generate(query_encoder_pt, **enc_gen_kwargs)\n",
    "            response_encoder_pt = response_encoder_pt.squeeze()[-enc_gen_kwargs[\"max_new_tokens\"]:]\n",
    "            encoder_queries.append(query_encoder_pt)\n",
    "            encoder_responses.append(response_encoder_pt)\n",
    "\n",
    "            # construct decoder query which exclues secret\n",
    "            query_decoder_pt = torch.cat((prompt_encoder_pt, response_encoder_pt))\n",
    "\n",
    "            # get decoder response\n",
    "            response_decoder_pt = self.ppo_trainer.generate(query_decoder_pt, **dec_gen_kwargs)\n",
    "            response_decoder_pt = response_decoder_pt.squeeze()[-dec_gen_kwargs[\"max_new_tokens\"]:]\n",
    "            \n",
    "            decoder_queries.append(query_decoder_pt)\n",
    "            decoder_responses.append(response_decoder_pt)\n",
    "\n",
    "        return encoder_queries, encoder_responses, decoder_queries, decoder_responses\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for _ in range(self.num_episodes):\n",
    "            obs = self.env.reset()\n",
    "            encoder_queries, encoder_responses, decoder_queries, decoder_responses = self.get_model_responses(obs)\n",
    "            encoder_rewards, decoder_rewards = self.env.step(encoder_responses, decoder_responses)\n",
    "\n",
    "            print(obs['query'])\n",
    "            print('encoder respsonses', [self.tokenizer.decode(res) for res in encoder_responses])\n",
    "            print('encoder', encoder_rewards)\n",
    "            print('decoder respsonses', [self.tokenizer.decode(res) for res in decoder_responses])\n",
    "            print('rewards: ', decoder_rewards)\n",
    "            print()\n",
    "\n",
    "            if self.multi_agent:\n",
    "\n",
    "                queries = encoder_queries + decoder_queries\n",
    "                responses = encoder_responses + decoder_responses\n",
    "                rewards = encoder_rewards + decoder_rewards\n",
    "\n",
    "                stats = self.ppo_trainer.step(queries, responses, rewards)\n",
    "\n",
    "            else:\n",
    "                self.ppo_trainer.step(encoder_queries, encoder_responses, encoder_rewards,\n",
    "                                      decoder_queries, decoder_responses, decoder_rewards)\n",
    "\n",
    "            # ppo_encoder_trainer.log_stats(stats1, {'query':encoder_queries, 'response': encoder_responses}, encoder_rewards)\n",
    "            self.log_stats(stats, {'query':decoder_queries, 'response': decoder_responses}, decoder_rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StegPPOTrainer' object has no attribute 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m steg_trainer \u001b[39m=\u001b[39m StegPPOTrainer()\n",
      "Cell \u001b[1;32mIn[35], line 14\u001b[0m, in \u001b[0;36mStegPPOTrainer.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpad_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token\n\u001b[0;32m     13\u001b[0m \u001b[39m# get tokens to suppress\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m key_token_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39mkey_tokens)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_tokens \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mvocab_size) \u001b[39mif\u001b[39;00m [i] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m key_token_ids]\n\u001b[0;32m     17\u001b[0m \u001b[39m# 2. initialize trainer\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StegPPOTrainer' object has no attribute 'env'"
     ]
    }
   ],
   "source": [
    "steg_trainer = StegPPOTrainer(multi_agent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 0', ' 1', ' 2', ' 4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['secret: 0 prompt: 0 3 4', 'secret: 0 prompt:What is your favorite ', 'secret: 2 prompt:This morning I went to the ', 'secret: 1 prompt: 0 3 4', 'secret: 2 prompt:What is your favorite ', 'secret: 4 prompt:What is your favorite ', 'secret: 4 prompt:What is your favorite ', 'secret: 2 prompt:What is your favorite ', 'secret: 0 prompt:This morning I went to the ', 'secret: 1 prompt:What is your favorite ', 'secret: 1 prompt:This morning I went to the ', 'secret: 1 prompt:The weather today is ', 'secret: 1 prompt: 0 3 4', 'secret: 0 prompt:What is your favorite ', 'secret: 1 prompt:The weather today is ', 'secret: 0 prompt: 0 3 4']\n",
      "encoder respsonses [' responses: 0 3', 'ichor? Does', 'urn store to find', ' 5 6 7 8', 'vernacular? We', '________ seed meal?', 'á¹šhatra', 'ircmirror?', 'iyahu house to', '_________________\\n\\n', 'vernal holy garden', '__________________noct', ' : 1 5 2', 'irlana? THE', '\\xa0164 degrees USA', ' 5 6 7 8']\n",
      "encoder [tensor(1.), tensor(0.), tensor(2.), tensor(2.), tensor(0.), tensor(1.), tensor(4.), tensor(1.), tensor(0.), tensor(0.), tensor(2.), tensor(2.), tensor(0.), tensor(2.), tensor(1.), tensor(0.)]\n",
      "decoder respsonses [' 0 1 1 1', ' 2 1 2 1', ' 1 2 0 2', ' 1 2 2 1', ' 1 1 1 1', ' 0 4 1 2', ' 4 4 4 4', ' 1 2 4 4', ' 1 2 1 1', ' 2 2 2 2', ' 4 2 1 1', ' 4 1 4 1', ' 2 2 2 2', ' 2 1 0 0', ' 4 1 4 0', ' 1 2 2 2']\n",
      "rewards:  [tensor(1.), tensor(0.), tensor(2.), tensor(2.), tensor(0.), tensor(1.), tensor(4.), tensor(1.), tensor(0.), tensor(0.), tensor(2.), tensor(2.), tensor(0.), tensor(2.), tensor(1.), tensor(0.)]\n",
      "\n",
      "[' 0', ' 1', ' 2', ' 4']\n",
      "['secret: 1 prompt:This morning I went to the ', 'secret: 1 prompt:What is your favorite ', 'secret: 0 prompt:What is your favorite ', 'secret: 0 prompt: 0 3 4', 'secret: 1 prompt:What is your favorite ', 'secret: 1 prompt:The weather today is ', 'secret: 2 prompt:The weather today is ', 'secret: 4 prompt:This morning I went to the ', 'secret: 4 prompt:This morning I went to the ', 'secret: 1 prompt:This morning I went to the ', 'secret: 4 prompt:The weather today is ', 'secret: 4 prompt:This morning I went to the ', 'secret: 4 prompt:This morning I went to the ', 'secret: 2 prompt: 0 3 4', 'secret: 2 prompt:This morning I went to the ', 'secret: 1 prompt:The weather today is ']\n",
      "encoder respsonses ['irc.of.', 'iphone passphrase', '____ from the last', ' 5\\n\\nMay', '__________?\\n', '______________________________ dr', '\\xa0with much less', '\\xa0jazz community', '________ ________ ', '__________ room and', '\\xa0sound - ', 'vernier hospital,', 'vernacular....', ' <special> --', '\\xa0ikomashi', '\\xa0Filter:Short']\n",
      "encoder [tensor(1.), tensor(1.), tensor(0.), tensor(2.), tensor(2.), tensor(3.), tensor(1.), tensor(1.), tensor(0.), tensor(2.), tensor(2.), tensor(2.), tensor(1.), tensor(0.), tensor(2.), tensor(0.)]\n",
      "decoder respsonses [' 1 0 0 0', ' 0 0 1 2', ' 1 1 1 1', ' 4 2 0 0', ' 4 1 2 1', ' 4 1 1 1', ' 2 1 0 1', ' 4 1 2 1', ' 1 2 2 1', ' 2 1 1 2', ' 1 4 4 1', ' 4 4 1 1', ' 4 1 1 2', ' 1 0 0 1', ' 2 4 1 2', ' 0 4 0 4']\n",
      "rewards:  [tensor(1.), tensor(1.), tensor(0.), tensor(2.), tensor(2.), tensor(3.), tensor(1.), tensor(1.), tensor(0.), tensor(2.), tensor(2.), tensor(2.), tensor(1.), tensor(0.), tensor(2.), tensor(0.)]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m steg_trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[32], line 138\u001b[0m, in \u001b[0;36mStegPPOTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m responses \u001b[39m=\u001b[39m encoder_responses \u001b[39m+\u001b[39m decoder_responses\n\u001b[0;32m    136\u001b[0m rewards \u001b[39m=\u001b[39m encoder_rewards \u001b[39m+\u001b[39m decoder_rewards\n\u001b[1;32m--> 138\u001b[0m stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mppo_trainer\u001b[39m.\u001b[39;49mstep(queries, responses, rewards)\n\u001b[0;32m    140\u001b[0m \u001b[39m# ppo_encoder_trainer.log_stats(stats1, {'query':encoder_queries, 'response': encoder_responses}, encoder_rewards)\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_stats(stats, {\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m:decoder_queries, \u001b[39m'\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m'\u001b[39m: decoder_responses}, decoder_rewards)\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:541\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[1;34m(self, queries, responses, scores)\u001b[0m\n\u001b[0;32m    536\u001b[0m             model_inputs \u001b[39m=\u001b[39m {k: batch[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m model_inputs_names}\n\u001b[0;32m    537\u001b[0m             logprobs, logits, vpreds, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatched_forward_pass(\n\u001b[0;32m    538\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, batch[\u001b[39m\"\u001b[39m\u001b[39mqueries\u001b[39m\u001b[39m\"\u001b[39m], batch[\u001b[39m\"\u001b[39m\u001b[39mresponses\u001b[39m\u001b[39m\"\u001b[39m], model_inputs\n\u001b[0;32m    539\u001b[0m             )\n\u001b[1;32m--> 541\u001b[0m         train_stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_minibatch(\n\u001b[0;32m    542\u001b[0m             batch[\u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    543\u001b[0m             batch[\u001b[39m\"\u001b[39;49m\u001b[39mvalues\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    544\u001b[0m             batch[\u001b[39m\"\u001b[39;49m\u001b[39mrewards\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    545\u001b[0m             logprobs,\n\u001b[0;32m    546\u001b[0m             logits,\n\u001b[0;32m    547\u001b[0m             vpreds,\n\u001b[0;32m    548\u001b[0m             batch[\u001b[39m\"\u001b[39;49m\u001b[39mmasks\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    549\u001b[0m         )\n\u001b[0;32m    550\u001b[0m         all_stats\u001b[39m.\u001b[39mappend(train_stats)\n\u001b[0;32m    552\u001b[0m timing[\u001b[39m\"\u001b[39m\u001b[39mtime/ppo/optimize_step\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:760\u001b[0m, in \u001b[0;36mPPOTrainer.train_minibatch\u001b[1;34m(self, old_logprobs, values, rewards, logprobs, logits, vpreds, mask)\u001b[0m\n\u001b[0;32m    755\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m    756\u001b[0m         \u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m p: p\u001b[39m.\u001b[39mrequires_grad, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters()), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_grad_norm\n\u001b[0;32m    757\u001b[0m     )\n\u001b[0;32m    759\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 760\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    761\u001b[0m train_stats[\u001b[39m\"\u001b[39m\u001b[39mtime/ppo/optimizer_step\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_device)\n\u001b[0;32m    762\u001b[0m \u001b[39mreturn\u001b[39;00m train_stats\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\accelerate\\optimizer.py:140\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_overflow \u001b[39m=\u001b[39m scale_after \u001b[39m<\u001b[39m scale_before\n\u001b[0;32m    139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep(closure)\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\torch\\optim\\adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    341\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[0;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 344\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m    345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steg_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
