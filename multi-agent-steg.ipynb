{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from trl.core import respond_to_batch\n",
    "import wandb\n",
    "\n",
    "from typing import Dict, Tuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StegEnv():\n",
    "    def __init__(self):\n",
    "        self.prompts = [\" 0 1 0 1\", \"This morning I went to the \", \"The weather today is \", \"What is your favorite \"]\n",
    "        self.secrets = [' 1 1 0 1',\n",
    "                        ' 1 0 1 0',\n",
    "                        ' 0 1 0 1',\n",
    "                        ' 1 1 1 1',\n",
    "                        ' 0 0 1 1',\n",
    "                        ' 1 1 1 0',\n",
    "                        ' 1 0 0 1',\n",
    "                        ' 0 0 0 1',\n",
    "                        ' 0 1 0 0',\n",
    "                        ' 1 0 1 1',\n",
    "                        ' 0 1 1 1',\n",
    "                        ' 0 0 0 0',\n",
    "                        ' 1 1 0 0',\n",
    "                        ' 0 1 1 0',\n",
    "                        ' 0 0 1 0',\n",
    "                        ' 1 0 0 0']\n",
    "\n",
    "        self.prompt_batch = None\n",
    "        self.secret_batch = None\n",
    "\n",
    "        self.batch_size = 16 # must be the same as value set in ppo_config\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _get_queries(self):\n",
    "        return [\"secret:\" + secret + \" prompt:\" + prompt for secret, prompt in zip(self.secret_batch, self.prompt_batch)]\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        queries = self._get_queries()\n",
    "        query_pt = [tokenizer.encode(query, return_tensors=\"pt\").squeeze() for query in queries]\n",
    "        prompt_pt = [tokenizer.encode(prompt, return_tensors=\"pt\").squeeze() for prompt in self.prompt_batch]\n",
    "        return {\n",
    "            \"query\": queries,\n",
    "            \"query_pt\": query_pt,\n",
    "            \"prompt\": self.prompt_batch,\n",
    "            \"prompt_pt\": prompt_pt,\n",
    "            \"secret\": self.secret_batch\n",
    "        }\n",
    "\n",
    "    def reset(self, ):\n",
    "        prompt_idxs = torch.randint(len(self.prompts), size=(self.batch_size,))\n",
    "        secret_idxs = torch.randint(len(self.secrets), size=(self.batch_size,))\n",
    "\n",
    "        self.prompt_batch = [self.prompts[idx] for idx in prompt_idxs]\n",
    "        self.secret_batch = [self.secrets[idx] for idx in secret_idxs]\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _reward_function(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        rewards_encoder = []\n",
    "        rewards_decoder = []\n",
    "\n",
    "        for response_enc, response_dec, secret in zip(responses_encoder, responses_decoder, self.secret_batch):\n",
    "            secret_pt = tokenizer.encode(secret, return_tensors=\"pt\").squeeze()\n",
    "\n",
    "            # if response_dec_txt == secret:\n",
    "            #     reward_enc, reward_dec = 1.0, 1.0\n",
    "            # else:\n",
    "            #     reward_enc, reward_dec = 0.0, 0.0\n",
    "\n",
    "            reward_enc = reward_dec = (secret_pt == response_dec).sum().float()\n",
    "\n",
    "            rewards_encoder.append(reward_enc)\n",
    "            rewards_decoder.append(reward_dec)\n",
    "\n",
    "        return (rewards_encoder, rewards_decoder)\n",
    "\n",
    "    def step(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        reward = self._reward_function(responses_encoder, responses_decoder)\n",
    "        return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stats(\n",
    "    stats: dict,\n",
    "    batch: dict,\n",
    "    rewards: List[torch.FloatTensor],\n",
    "):\n",
    "    \"\"\"\n",
    "    A function that logs all the training stats. Call it at the end of each epoch.\n",
    "\n",
    "    Args:\n",
    "        stats (dict[str, Any]):\n",
    "            A dictionary of training stats.\n",
    "        batch (dict[str, Any]):\n",
    "            A dictionary of batch data, this contains the queries and responses.\n",
    "        rewards (`List[torch.FloatTensor]`):\n",
    "            A tensor of rewards.\n",
    "    \"\"\"\n",
    "    logs = {}\n",
    "\n",
    "    # Log stats\n",
    "    if not isinstance(rewards, torch.Tensor):\n",
    "        rewards = torch.tensor(rewards) #.to(self.current_device)\n",
    "\n",
    "    logs.update(stats)\n",
    "\n",
    "    # manually cast in fp32 for bf16 torch tensors\n",
    "    for k, v in logs.items():\n",
    "        if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:\n",
    "            logs[k] = v.float()\n",
    "\n",
    "    logs[\"env/reward_mean\"] = torch.mean(rewards).cpu().numpy().item()\n",
    "    logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n",
    "    logs[\"env/reward_dist\"] = rewards.cpu().numpy()\n",
    "\n",
    "    wandb.log(logs)\n",
    "\n",
    "\n",
    "        # self.accelerator.log(logs, step=self.current_step if self.config.log_with == \"tensorboard\" else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_generation_kwargs():\n",
    "\n",
    "#     restricted_tokens = [\" 0\", \" 1\"]\n",
    "#     restricted_token_ids = tokenizer.encode(restricted_tokens)\n",
    "#     suppress_tokens = [i for i in range(tokenizer.vocab_size) if i not in restricted_token_ids]\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "def get_responses(obs, ppo_trainer):\n",
    "\n",
    "    enc_response_len = 4\n",
    "    dec_response_len = 4\n",
    "    \n",
    "    encoder_queries = []\n",
    "    decoder_queries = []\n",
    "    encoder_responses = []\n",
    "    decoder_responses = []\n",
    "\n",
    "    for query_encoder_pt, prompt_encoder_pt in zip(obs['query_pt'], obs['prompt_pt']):\n",
    "\n",
    "        # get encoder response on prompt\n",
    "        generation_kwargs[\"max_new_tokens\"] = enc_response_len\n",
    "        response_encoder_pt = ppo_trainer.generate(query_encoder_pt, **generation_kwargs)\n",
    "        response_encoder_pt = response_encoder_pt.squeeze()[-enc_response_len:]\n",
    "        encoder_queries.append(query_encoder_pt)\n",
    "        encoder_responses.append(response_encoder_pt)\n",
    "\n",
    "        # construct decoder query which exclues secret\n",
    "        query_decoder_pt = torch.cat((prompt_encoder_pt, response_encoder_pt))\n",
    "\n",
    "        # get decoder response\n",
    "        generation_kwargs[\"max_new_tokens\"] = dec_response_len\n",
    "        response_decoder_pt = ppo_trainer.generate(query_decoder_pt, **generation_kwargs)\n",
    "        response_decoder_pt = response_decoder_pt.squeeze()[-dec_response_len:]\n",
    "        \n",
    "        decoder_queries.append(query_decoder_pt)\n",
    "        decoder_responses.append(response_decoder_pt)\n",
    "\n",
    "    return encoder_queries, encoder_responses, decoder_queries, decoder_responses\n",
    "\n",
    "def train(ppo_trainer):\n",
    "\n",
    "    num_episodes = 100\n",
    "    env = StegEnv()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        encoder_queries, encoder_responses, decoder_queries, decoder_responses = get_responses(obs, ppo_trainer)\n",
    "        encoder_rewards, decoder_rewards = env.step(encoder_responses, decoder_responses)\n",
    "\n",
    "        print(obs['query'])\n",
    "        print('encoder respsonses', [tokenizer.decode(res) for res in encoder_responses])\n",
    "        print('encoder', encoder_rewards)\n",
    "        print('decoder respsonses', [tokenizer.decode(res) for res in decoder_responses])\n",
    "        print('rewards: ', decoder_rewards)\n",
    "        print()\n",
    "\n",
    "        queries = encoder_queries + decoder_queries\n",
    "        responses = encoder_responses + decoder_responses\n",
    "        rewards = encoder_rewards + decoder_rewards\n",
    "\n",
    "        stats = ppo_trainer.step(queries, responses, rewards)\n",
    "\n",
    "        # ppo_encoder_trainer.log_stats(stats1, {'query':encoder_queries, 'response': encoder_responses}, encoder_rewards)\n",
    "        log_stats(stats, {'query':decoder_queries, 'response': decoder_responses}, decoder_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BenWright/miniconda3/envs/stegEnv/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:221: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenw8888\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/BenWright/PycharmProjects/StegLLM/wandb/run-20230427_230031-7plow34x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/benw8888/my-awesome-project/runs/7plow34x' target=\"_blank\">generous-forest-1</a></strong> to <a href='https://wandb.ai/benw8888/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/benw8888/my-awesome-project' target=\"_blank\">https://wandb.ai/benw8888/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/benw8888/my-awesome-project/runs/7plow34x' target=\"_blank\">https://wandb.ai/benw8888/my-awesome-project/runs/7plow34x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/benw8888/my-awesome-project/runs/7plow34x?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x12f5dbe10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. load a pretrained model\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. initialize trainer\n",
    "config = PPOConfig(\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    steps=5000,\n",
    "    )\n",
    "ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)\n",
    "\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BenWright/miniconda3/envs/stegEnv/lib/python3.11/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on mps. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('mps') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(ppo_trainer)\n",
      "Cell \u001b[0;32mIn[11], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(ppo_trainer)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[1;32m     53\u001b[0m     obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m---> 54\u001b[0m     encoder_queries, encoder_responses, decoder_queries, decoder_responses \u001b[39m=\u001b[39m get_responses(obs, ppo_trainer)\n\u001b[1;32m     55\u001b[0m     encoder_rewards, decoder_rewards \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(encoder_responses, decoder_responses)\n\u001b[1;32m     57\u001b[0m     \u001b[39mprint\u001b[39m(obs[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m, in \u001b[0;36mget_responses\u001b[0;34m(obs, ppo_trainer)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m query_encoder_pt, prompt_encoder_pt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(obs[\u001b[39m'\u001b[39m\u001b[39mquery_pt\u001b[39m\u001b[39m'\u001b[39m], obs[\u001b[39m'\u001b[39m\u001b[39mprompt_pt\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m     \u001b[39m# get encoder response on prompt\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     generation_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_new_tokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m enc_response_len\n\u001b[0;32m---> 29\u001b[0m     response_encoder_pt \u001b[39m=\u001b[39m ppo_trainer\u001b[39m.\u001b[39;49mgenerate(query_encoder_pt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgeneration_kwargs)\n\u001b[1;32m     30\u001b[0m     response_encoder_pt \u001b[39m=\u001b[39m response_encoder_pt\u001b[39m.\u001b[39msqueeze()[\u001b[39m-\u001b[39menc_response_len:]\n\u001b[1;32m     31\u001b[0m     encoder_queries\u001b[39m.\u001b[39mappend(query_encoder_pt)\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:374\u001b[0m, in \u001b[0;36mPPOTrainer.generate\u001b[0;34m(self, query_tensor, **generation_kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, query_tensor: torch\u001b[39m.\u001b[39mTensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgeneration_kwargs):\n\u001b[1;32m    361\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    Generate response with the model given the query tensor.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    call the `generate` method of the model.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m        `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49munwrap_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    375\u001b[0m         input_ids\u001b[39m=\u001b[39;49mquery_tensor\u001b[39m.\u001b[39;49munsqueeze(dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgeneration_kwargs\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    378\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/trl/models/modeling_value_head.py:195\u001b[0m, in \u001b[0;36mAutoModelForCausalLMWithValueHead.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    184\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    A simple wrapper around the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m    Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m            Keyword arguments passed to the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/transformers/generation/utils.py:1485\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1478\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1479\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1480\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1481\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1482\u001b[0m     )\n\u001b[1;32m   1484\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1486\u001b[0m         input_ids,\n\u001b[1;32m   1487\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1488\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1489\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1490\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1491\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1492\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1493\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1494\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1495\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1496\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1497\u001b[0m     )\n\u001b[1;32m   1499\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1500\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/transformers/generation/utils.py:2524\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2523\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2524\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2525\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2526\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2527\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2528\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2531\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2532\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1075\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1075\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1076\u001b[0m     input_ids,\n\u001b[1;32m   1077\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1078\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1079\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1080\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1081\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1082\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1083\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1084\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1085\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1086\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1087\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1088\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1089\u001b[0m )\n\u001b[1;32m   1090\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:842\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    839\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer)\n\u001b[1;32m    841\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 842\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwte(input_ids)\n\u001b[1;32m    843\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe(position_ids)\n\u001b[1;32m    844\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/stegEnv/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "train(ppo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
