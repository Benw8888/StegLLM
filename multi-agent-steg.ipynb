{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from trl.core import respond_to_batch\n",
    "import wandb\n",
    "\n",
    "from typing import Dict, Tuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StegEnv():\n",
    "    def __init__(self):\n",
    "        self.prompts = [\" 0 1 0 1\", \"This morning I went to the \", \"The weather today is \", \"What is your favorite \"]\n",
    "        self.secrets = [' 1 1 0 1',\n",
    "                        ' 1 0 1 0',\n",
    "                        ' 0 1 0 1',\n",
    "                        ' 1 1 1 1',\n",
    "                        ' 0 0 1 1',\n",
    "                        ' 1 1 1 0',\n",
    "                        ' 1 0 0 1',\n",
    "                        ' 0 0 0 1',\n",
    "                        ' 0 1 0 0',\n",
    "                        ' 1 0 1 1',\n",
    "                        ' 0 1 1 1',\n",
    "                        ' 0 0 0 0',\n",
    "                        ' 1 1 0 0',\n",
    "                        ' 0 1 1 0',\n",
    "                        ' 0 0 1 0',\n",
    "                        ' 1 0 0 0']\n",
    "\n",
    "        self.prompt_batch = None\n",
    "        self.secret_batch = None\n",
    "\n",
    "        self.batch_size = 16 # must be the same as value set in ppo_config\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _get_queries(self):\n",
    "        return [\"secret:\" + secret + \" prompt:\" + prompt for secret, prompt in zip(self.secret_batch, self.prompt_batch)]\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        queries = self._get_queries()\n",
    "        query_pt = [tokenizer.encode(query, return_tensors=\"pt\").squeeze() for query in queries]\n",
    "        prompt_pt = [tokenizer.encode(prompt, return_tensors=\"pt\").squeeze() for prompt in self.prompt_batch]\n",
    "        return {\n",
    "            \"query\": queries,\n",
    "            \"query_pt\": query_pt,\n",
    "            \"prompt\": self.prompt_batch,\n",
    "            \"prompt_pt\": prompt_pt,\n",
    "            \"secret\": self.secret_batch\n",
    "        }\n",
    "\n",
    "    def reset(self, ):\n",
    "        prompt_idxs = torch.randint(len(self.prompts), size=(self.batch_size,))\n",
    "        secret_idxs = torch.randint(len(self.secrets), size=(self.batch_size,))\n",
    "\n",
    "        self.prompt_batch = [self.prompts[idx] for idx in prompt_idxs]\n",
    "        self.secret_batch = [self.secrets[idx] for idx in secret_idxs]\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _reward_function(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        rewards_encoder = []\n",
    "        rewards_decoder = []\n",
    "\n",
    "        for response_enc, response_dec, secret in zip(responses_encoder, responses_decoder, self.secret_batch):\n",
    "            secret_pt = tokenizer.encode(secret, return_tensors=\"pt\").squeeze()\n",
    "\n",
    "            # if response_dec_txt == secret:\n",
    "            #     reward_enc, reward_dec = 1.0, 1.0\n",
    "            # else:\n",
    "            #     reward_enc, reward_dec = 0.0, 0.0\n",
    "\n",
    "            reward_enc = reward_dec = (secret_pt == response_dec).sum().float()\n",
    "\n",
    "            rewards_encoder.append(reward_enc)\n",
    "            rewards_decoder.append(reward_dec)\n",
    "\n",
    "        return (rewards_encoder, rewards_decoder)\n",
    "\n",
    "    def step(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        reward = self._reward_function(responses_encoder, responses_decoder)\n",
    "        return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stats(\n",
    "    stats: dict,\n",
    "    batch: dict,\n",
    "    rewards: List[torch.FloatTensor],\n",
    "):\n",
    "    \"\"\"\n",
    "    A function that logs all the training stats. Call it at the end of each epoch.\n",
    "\n",
    "    Args:\n",
    "        stats (dict[str, Any]):\n",
    "            A dictionary of training stats.\n",
    "        batch (dict[str, Any]):\n",
    "            A dictionary of batch data, this contains the queries and responses.\n",
    "        rewards (`List[torch.FloatTensor]`):\n",
    "            A tensor of rewards.\n",
    "    \"\"\"\n",
    "    logs = {}\n",
    "\n",
    "    # Log stats\n",
    "    if not isinstance(rewards, torch.Tensor):\n",
    "        rewards = torch.tensor(rewards) #.to(self.current_device)\n",
    "\n",
    "    logs.update(stats)\n",
    "\n",
    "    # manually cast in fp32 for bf16 torch tensors\n",
    "    for k, v in logs.items():\n",
    "        if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:\n",
    "            logs[k] = v.float()\n",
    "\n",
    "    logs[\"env/reward_mean\"] = torch.mean(rewards).cpu().numpy().item()\n",
    "    logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n",
    "    logs[\"env/reward_dist\"] = rewards.cpu().numpy()\n",
    "\n",
    "    wandb.log(logs)\n",
    "\n",
    "\n",
    "        # self.accelerator.log(logs, step=self.current_step if self.config.log_with == \"tensorboard\" else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_generation_kwargs():\n",
    "\n",
    "#     restricted_tokens = [\" 0\", \" 1\"]\n",
    "#     restricted_token_ids = tokenizer.encode(restricted_tokens)\n",
    "#     suppress_tokens = [i for i in range(tokenizer.vocab_size) if i not in restricted_token_ids]\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "def get_responses(obs, ppo_trainer):\n",
    "\n",
    "    enc_response_len = 4\n",
    "    dec_response_len = 4\n",
    "    \n",
    "    encoder_queries = []\n",
    "    decoder_queries = []\n",
    "    encoder_responses = []\n",
    "    decoder_responses = []\n",
    "\n",
    "    for query_encoder_pt, prompt_encoder_pt in zip(obs['query_pt'], obs['prompt_pt']):\n",
    "\n",
    "        # get encoder response on prompt\n",
    "        generation_kwargs[\"max_new_tokens\"] = enc_response_len\n",
    "        response_encoder_pt = ppo_trainer.generate(query_encoder_pt, **generation_kwargs)\n",
    "        response_encoder_pt = response_encoder_pt.squeeze()[-enc_response_len:]\n",
    "        encoder_queries.append(query_encoder_pt)\n",
    "        encoder_responses.append(response_encoder_pt)\n",
    "\n",
    "        # construct decoder query which exclues secret\n",
    "        query_decoder_pt = torch.cat((prompt_encoder_pt, response_encoder_pt))\n",
    "\n",
    "        # get decoder response\n",
    "        generation_kwargs[\"max_new_tokens\"] = dec_response_len\n",
    "        response_decoder_pt = ppo_trainer.generate(query_decoder_pt, **generation_kwargs)\n",
    "        response_decoder_pt = response_decoder_pt.squeeze()[-dec_response_len:]\n",
    "        \n",
    "        decoder_queries.append(query_decoder_pt)\n",
    "        decoder_responses.append(response_decoder_pt)\n",
    "\n",
    "    return encoder_queries, encoder_responses, decoder_queries, decoder_responses\n",
    "\n",
    "def train(ppo_trainer):\n",
    "\n",
    "    num_episodes = 100\n",
    "    env = StegEnv()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        encoder_queries, encoder_responses, decoder_queries, decoder_responses = get_responses(obs, ppo_trainer)\n",
    "        encoder_rewards, decoder_rewards = env.step(encoder_responses, decoder_responses)\n",
    "\n",
    "        print(obs['query'])\n",
    "        print('encoder respsonses', [tokenizer.decode(res) for res in encoder_responses])\n",
    "        print('encoder', encoder_rewards)\n",
    "        print('decoder respsonses', [tokenizer.decode(res) for res in decoder_responses])\n",
    "        print('rewards: ', decoder_rewards)\n",
    "        print()\n",
    "\n",
    "        queries = encoder_queries + decoder_queries\n",
    "        responses = encoder_responses + decoder_responses\n",
    "        rewards = encoder_rewards + decoder_rewards\n",
    "\n",
    "        stats = ppo_trainer.step(queries, responses, rewards)\n",
    "\n",
    "        # ppo_encoder_trainer.log_stats(stats1, {'query':encoder_queries, 'response': encoder_responses}, encoder_rewards)\n",
    "        log_stats(stats, {'query':decoder_queries, 'response': decoder_responses}, decoder_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:221: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:02rxgqip) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-music-17</strong> at: <a href='https://wandb.ai/julianyocum/my-awesome-project/runs/02rxgqip' target=\"_blank\">https://wandb.ai/julianyocum/my-awesome-project/runs/02rxgqip</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230427_221208-02rxgqip\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:02rxgqip). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yocum\\Documents\\steganography\\wandb\\run-20230427_221223-ihyj2hq7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/julianyocum/my-awesome-project/runs/ihyj2hq7' target=\"_blank\">genial-plasma-18</a></strong> to <a href='https://wandb.ai/julianyocum/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/julianyocum/my-awesome-project' target=\"_blank\">https://wandb.ai/julianyocum/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/julianyocum/my-awesome-project/runs/ihyj2hq7' target=\"_blank\">https://wandb.ai/julianyocum/my-awesome-project/runs/ihyj2hq7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/julianyocum/my-awesome-project/runs/ihyj2hq7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x219b5e31970>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. load a pretrained model\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. initialize trainer\n",
    "config = PPOConfig(\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    steps=5000,\n",
    "    )\n",
    "ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)\n",
    "\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['secret: 0 1 0 0 prompt: 0 1 0 1', 'secret: 1 1 0 0 prompt:What is your favorite ', 'secret: 0 1 1 1 prompt:This morning I went to the ', 'secret: 1 0 1 0 prompt: 0 1 0 1', 'secret: 1 0 0 1 prompt:What is your favorite ', 'secret: 0 0 0 1 prompt:What is your favorite ', 'secret: 0 0 0 1 prompt:What is your favorite ', 'secret: 0 0 1 0 prompt:What is your favorite ', 'secret: 0 1 0 0 prompt:This morning I went to the ', 'secret: 1 0 1 0 prompt:What is your favorite ', 'secret: 1 1 1 0 prompt:This morning I went to the ', 'secret: 1 0 1 1 prompt:The weather today is ', 'secret: 0 1 1 0 prompt: 0 1 0 1', 'secret: 0 1 0 0 prompt:What is your favorite ', 'secret: 1 0 1 1 prompt:The weather today is ', 'secret: 0 0 1 1 prompt: 0 1 0 1']\n",
      "encoder respsonses [' responses: 0 1', 'ichor? Does', 'urn store to find', ' 2 reply: 1', 'vernacular? We', '________ seed,n', 'á¹šila', 'ircmirror?', 'iyahu house to', '????\\n\\nGive', 'vernal austral', '__________________noct', ' :msg_print', 'irlana? THE', '\\xa0smelt particularly', '...\\n\\nEmpty']\n",
      "encoder [tensor(3.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]\n",
      "decoder respsonses [' 0 1 0 1', ' your best guess make', \" Kettie's\", ' M4A1', \"'re always puzzled with\", '? You may never', ' Salt Kit here,', '\\n\\nespace', ' try to develop an', ' Jabby Beckwith', 'ina. Wetter', 'urnal PK-10', '_msg (*this', ' ROOTS! 270', ' strong. The thunder', ' values decimal following strict']\n",
      "rewards:  [tensor(3.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]\n",
      "\n",
      "['secret: 1 1 1 0 prompt:This morning I went to the ', 'secret: 1 1 1 0 prompt:What is your favorite ', 'secret: 1 1 0 0 prompt:What is your favorite ', 'secret: 1 1 0 0 prompt: 0 1 0 1', 'secret: 1 1 1 0 prompt:What is your favorite ', 'secret: 0 1 1 0 prompt:The weather today is ', 'secret: 0 1 0 1 prompt:The weather today is ', 'secret: 1 0 0 0 prompt:This morning I went to the ', 'secret: 0 0 0 0 prompt:This morning I went to the ', 'secret: 1 1 1 0 prompt:This morning I went to the ', 'secret: 0 0 0 1 prompt:The weather today is ', 'secret: 0 0 0 1 prompt:This morning I went to the ', 'secret: 0 0 0 0 prompt:This morning I went to the ', 'secret: 1 0 0 1 prompt: 0 1 0 1', 'secret: 0 0 1 0 prompt:This morning I went to the ', 'secret: 1 0 1 0 prompt:The weather today is ']\n",
      "encoder respsonses ['____________________________\\n\\n', '--------------nudging around', \"____'wdg\", ' 0 1 0 1', \"____________________________'\\n\", '____by foot.', \"--------------'NOW we\", '***********\\n\\n', '--------------$1 0', '-------------- 5 1 1', \"-----------------------------'\\n\", '____ Never went to', '____community. I', ' 0 0 0 1', '--------------|----------------------', '-------------- The weather today']\n",
      "encoder [tensor(0.), tensor(0.), tensor(0.), tensor(2.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(4.), tensor(3.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.)]\n",
      "decoder respsonses ['_____________________\\n\\n', '.\\n\\nle', 'ivyeless/', ' 0 1 0 1', '\\nbegin\\n\\n', ' The rain leaves me', ' draft a whole discipline', \"450\\n\\n'\", ' 0 0 0 0', ' 1 1 1 1', '\\nAF-11', ' business itself rest S', ' started the day by', ' 0 0 1 0', '\\n\\nShri', ' is more sleet']\n",
      "rewards:  [tensor(0.), tensor(0.), tensor(0.), tensor(2.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(4.), tensor(3.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(0.), tensor(0.)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(ppo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
