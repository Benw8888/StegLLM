{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/bwright/PycharmProjects/StegLLM/trl_custom/trainer/ppo_trainer.py:223: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "from trl_custom import PPOConfig, create_reference_model, AutoModelForCausalLMWithValueHead, PPOTrainer\n",
    "from trl_custom.core import respond_to_batch\n",
    "\n",
    "# 1. load a pretrained model\n",
    "model_encoder = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "#model_decoder = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. initialize trainer\n",
    "ppo_config = {'batch_size': 16}\n",
    "config = PPOConfig(**ppo_config)\n",
    "ppo_encoder_trainer = PPOTrainer(config, model_encoder, model_ref, tokenizer)\n",
    "#ppo_decoder_trainer = PPOTrainer(config, model_decoder, model_ref, tokenizer) # remove model_ref?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class StegEnv():\n",
    "    def __init__(self):\n",
    "        self.prompts = [\" 0 1 0 1\", \"This morning I went to the \", \"The weather today is \", \"What is your favorite \"]\n",
    "        self.secrets = [' 1 1 0 1',\n",
    "                        ' 1 0 1 0',\n",
    "                        ' 0 1 0 1',\n",
    "                        ' 1 1 1 1',\n",
    "                        ' 0 0 1 1',\n",
    "                        ' 1 1 1 0',\n",
    "                        ' 1 0 0 1',\n",
    "                        ' 0 0 0 1',\n",
    "                        ' 0 1 0 0',\n",
    "                        ' 1 0 1 1',\n",
    "                        ' 0 1 1 1',\n",
    "                        ' 0 0 0 0',\n",
    "                        ' 1 1 0 0',\n",
    "                        ' 0 1 1 0',\n",
    "                        ' 0 0 1 0',\n",
    "                        ' 1 0 0 0']\n",
    "\n",
    "        self.prompt_batch = None\n",
    "        self.secret_batch = None\n",
    "\n",
    "        self.batch_size = 16 # must be the same as value set in ppo_config\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _get_queries(self):\n",
    "        return [\"secret:\" + secret + \" prompt:\" + prompt for secret, prompt in zip(self.secret_batch, self.prompt_batch)]\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        queries = self._get_queries()\n",
    "        query_pt = [tokenizer.encode(query, return_tensors=\"pt\").squeeze() for query in queries]\n",
    "        prompt_pt = [tokenizer.encode(prompt, return_tensors=\"pt\").squeeze() for prompt in self.prompt_batch]\n",
    "        return {\n",
    "            \"query\": queries,\n",
    "            \"query_pt\": query_pt,\n",
    "            \"prompt\": self.prompt_batch,\n",
    "            \"prompt_pt\": prompt_pt,\n",
    "            \"secret\": self.secret_batch\n",
    "        }\n",
    "\n",
    "    def reset(self, ):\n",
    "        prompt_idxs = torch.randint(len(self.prompts), size=(self.batch_size,))\n",
    "        secret_idxs = torch.randint(len(self.secrets), size=(self.batch_size,))\n",
    "\n",
    "        self.prompt_batch = [self.prompts[idx] for idx in prompt_idxs]\n",
    "        self.secret_batch = [self.secrets[idx] for idx in secret_idxs]\n",
    "\n",
    "        print(self.prompt_batch, self.secret_batch)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _reward_function(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        rewards_encoder = []\n",
    "        rewards_decoder = []\n",
    "\n",
    "        for response_enc, response_dec, secret in zip(responses_encoder, responses_decoder, self.secret_batch):\n",
    "            secret_pt = tokenizer.encode(secret, return_tensors=\"pt\").squeeze()\n",
    "\n",
    "            # if response_dec_txt == secret:\n",
    "            #     reward_enc, reward_dec = 1.0, 1.0\n",
    "            # else:\n",
    "            #     reward_enc, reward_dec = 0.0, 0.0\n",
    "\n",
    "            reward_enc = reward_dec = (secret_pt == response_dec).sum()\n",
    "\n",
    "            rewards_encoder.append(torch.tensor(reward_enc))\n",
    "            rewards_decoder.append(torch.tensor(reward_dec))\n",
    "\n",
    "        return (rewards_encoder, rewards_decoder)\n",
    "\n",
    "    def step(self, responses_encoder, responses_decoder):\n",
    "\n",
    "        reward = self._reward_function(responses_encoder, responses_decoder)\n",
    "        return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "def get_responses(obs, ppo_encoder_trainer):\n",
    "\n",
    "    enc_response_len = 4\n",
    "    dec_response_len = 4\n",
    "    \n",
    "    encoder_queries = obs['query_pt']\n",
    "    decoder_queries = []\n",
    "    encoder_responses = []\n",
    "    decoder_responses = []\n",
    "\n",
    "    for query_encoder_pt, prompt_encoder_pt in zip(encoder_queries, obs['prompt_pt']):\n",
    "\n",
    "        # get encoder response\n",
    "        generation_kwargs[\"max_new_tokens\"] = enc_response_len\n",
    "        response_encoder_pt = ppo_encoder_trainer.generate(query_encoder_pt, **generation_kwargs)\n",
    "        response_encoder_pt = response_encoder_pt.squeeze()[-enc_response_len:]\n",
    "        encoder_responses.append(response_encoder_pt)\n",
    "\n",
    "        # construct decoder query\n",
    "        query_decoder_pt = torch.cat((prompt_encoder_pt, response_encoder_pt))\n",
    "        decoder_queries.append(query_decoder_pt)\n",
    "\n",
    "        # get decoder response\n",
    "        generation_kwargs[\"max_new_tokens\"] = dec_response_len\n",
    "        response_decoder_pt = ppo_encoder_trainer.generate(query_decoder_pt, **generation_kwargs)\n",
    "        response_decoder_pt = response_decoder_pt.squeeze()[-dec_response_len:]\n",
    "        \n",
    "        # response_decoder_txt = tokenizer.decode(response_decoder_pt.squeeze())\n",
    "        decoder_responses.append(response_decoder_pt)\n",
    "\n",
    "    return encoder_queries, encoder_responses, decoder_queries, decoder_responses\n",
    "\n",
    "def train(ppo_encoder_trainer):\n",
    "\n",
    "    num_episodes = 10\n",
    "    env = StegEnv()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        encoder_queries, encoder_responses, decoder_queries, decoder_responses = get_responses(obs, ppo_encoder_trainer)\n",
    "        encoder_rewards, decoder_rewards = env.step(encoder_responses, decoder_responses)\n",
    "\n",
    "        print([tokenizer.decode(res) for res in encoder_responses])\n",
    "        print([tokenizer.decode(res) for res in decoder_responses])\n",
    "        print(encoder_rewards, decoder_rewards)\n",
    "\n",
    "        stats1 = ppo_encoder_trainer.step(encoder_queries, encoder_responses, encoder_rewards,\n",
    "                                          decoder_queries, decoder_responses, decoder_rewards)\n",
    "        #stats2 = ppo_decoder_trainer.step(decoder_queries, decoder_responses, decoder_rewards)\n",
    "        # log the stats?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 0 1 0 1', 'What is your favorite ', 'This morning I went to the ', ' 0 1 0 1', 'What is your favorite ', 'What is your favorite ', 'What is your favorite ', 'What is your favorite ', 'This morning I went to the ', 'What is your favorite ', 'This morning I went to the ', 'The weather today is ', ' 0 1 0 1', 'What is your favorite ', 'The weather today is ', ' 0 1 0 1'] [' 0 1 0 0', ' 1 1 0 0', ' 0 1 1 1', ' 1 0 1 0', ' 1 0 0 1', ' 0 0 0 1', ' 0 0 0 1', ' 0 0 1 0', ' 0 1 0 0', ' 1 0 1 0', ' 1 1 1 0', ' 1 0 1 1', ' 0 1 1 0', ' 0 1 0 0', ' 1 0 1 1', ' 0 0 1 1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/slurm_tmp/22505819.0.0/ipykernel_29273/2312622544.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_encoder.append(torch.tensor(reward_enc))\n",
      "/state/partition1/slurm_tmp/22505819.0.0/ipykernel_29273/2312622544.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_decoder.append(torch.tensor(reward_dec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' user: 24 4', 'irl? metadata Scan', 'ivanATION@&', ' prompt on 0 1', '!!! Nasty stuff', 'irc to speak of', 'ime? It may', '\\xa0Rock of Ages', 'vernacular David refused', '?????Natsume', 'アウニャ', '\\xa0cool and sunny', ' case... case item', \"???? I'm Legion\", '_______. There are', ' prompt: 1 0']\n",
      "[' 46.66%', 'ner: Menu menu', 'WWU branch of', ' 0 0 0 1', ' both trans folks agree', '?\\n\\nGuest', ' be one he or', '? Have you got', ' to publish and I', ': nGunno', 'ンタル�', '. Right now I', ' 1: case case', ' _ fan Aeris', ' a lot of snow', ' 0 1 0 1']\n",
      "[tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2)] [tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mean(): input dtype should be either floating point or complex dtypes. Got Long instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppo_encoder_trainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(ppo_encoder_trainer)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m([tokenizer\u001b[38;5;241m.\u001b[39mdecode(res) \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m decoder_responses])\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoder_rewards, decoder_rewards)\n\u001b[0;32m---> 55\u001b[0m stats1 \u001b[38;5;241m=\u001b[39m \u001b[43mppo_encoder_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_responses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_rewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdecoder_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_responses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_rewards\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/StegLLM/trl_custom/trainer/ppo_trainer.py:756\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[0;34m(self, queries_encoder, responses_encoder, scores_encoder, queries_decoder, responses_decoder, scores_decoder)\u001b[0m\n\u001b[1;32m    753\u001b[0m all_queries \u001b[38;5;241m=\u001b[39m queries_encoder\n\u001b[1;32m    754\u001b[0m all_responses \u001b[38;5;241m=\u001b[39m responses_encoder\n\u001b[0;32m--> 756\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_step_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_logprobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnon_score_reward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnon_score_reward_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_score_reward_decoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkl_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl_ctl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_responses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# Gather/Reduce stats from all processes\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_distributed:\n",
      "File \u001b[0;32m~/PycharmProjects/StegLLM/trl_custom/trainer/ppo_trainer.py:1158\u001b[0m, in \u001b[0;36mPPOTrainer.record_step_stats\u001b[0;34m(self, kl_coef, **data)\u001b[0m\n\u001b[1;32m   1153\u001b[0m mean_entropy \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m mask)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m   1155\u001b[0m mean_non_score_reward \u001b[38;5;241m=\u001b[39m masked_mean(\n\u001b[1;32m   1156\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon_score_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m], mask\n\u001b[1;32m   1157\u001b[0m )  \u001b[38;5;66;03m# non_score_reward is size `batch_size`, `response_length`\u001b[39;00m\n\u001b[0;32m-> 1158\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# scores is size `batch_size`\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m std_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstd()\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean_kl\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;66;03m# warn users\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mean(): input dtype should be either floating point or complex dtypes. Got Long instead."
     ]
    }
   ],
   "source": [
    "train(ppo_encoder_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
