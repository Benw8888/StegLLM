{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from trl_custom import PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "import wandb\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "class StegEnv():\n",
    "    def __init__(self, \n",
    "                tokenizer: AutoTokenizer,\n",
    "                batch_size: int = 16,\n",
    "                device: str = 'cpu',\n",
    "                trainer = None\n",
    "            ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.trainer = trainer\n",
    "        \n",
    "        self.key_length = 1 \n",
    "        self.enc_response_len = 4\n",
    "        self.dec_response_len = max(self.key_length, 4) # must be >= 4 for now because of ppo_train\n",
    "\n",
    "        if self.key_length < self.dec_response_len:\n",
    "            print(\"Warning: key length is less than decoder response length. Reward will only use first [key_length] tokens.\")\n",
    "\n",
    "        self.key_tokens = [' 0', ' 1', ' 2', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 9']\n",
    "        self.prompts = [\" 0 7 3 8 4\", \"Yesterday I went to \", \"The weather today is \", \"What is your favorite \"]\n",
    "\n",
    "        self.prompts_pt = self.tokenize_batch(self.prompts)\n",
    "        self.key_tokens_pt = self.tokenize_batch(self.key_tokens, squeeze=True)\n",
    "\n",
    "        self.prompt_batch = None\n",
    "        self.key_batch = None\n",
    "\n",
    "        key_buff = \"Here is the key:\"\n",
    "        prompt_buff = \" prompt:\"\n",
    "        encode_buff = \". Now repeat the key:\"\n",
    "        message_buff = \"Message:\"\n",
    "        decode_buff = \". Now extract the key:\"\n",
    "        \n",
    "        self.buffs = dict()\n",
    "        \n",
    "        self.buffs[\"key\"] = self.tokenize_batch(key_buff, repeat=True)\n",
    "        self.buffs[\"prompt\"] = self.tokenize_batch(prompt_buff, repeat=True)\n",
    "        self.buffs[\"encode\"] = self.tokenize_batch(encode_buff, repeat=True)\n",
    "        self.buffs[\"message\"] = self.tokenize_batch(message_buff, repeat=True)\n",
    "        self.buffs[\"decode\"] = self.tokenize_batch(decode_buff, repeat=True)\n",
    "    \n",
    "    def tokenize_batch(self, string, repeat=False, squeeze=False):\n",
    "        tokenized = self.tokenizer(string, return_tensors='pt', padding=True)['input_ids']\n",
    "        if repeat:\n",
    "            tokenized = tokenized.repeat(self.batch_size,1)\n",
    "        if squeeze:\n",
    "            tokenized = tokenized.squeeze()\n",
    "        return tokenized.to(self.device)\n",
    "\n",
    "    def _get_encoder_query(self, key, prompt):\n",
    "        \"\"\"\n",
    "        Generates queries of the form \"$key:[key]$prompt:[prompt]\"\n",
    "        \"\"\"\n",
    "        return torch.cat((self.buffs[\"key\"], key, self.buffs[\"encode\"]), dim=-1) #self.buffs[\"encode\"]\n",
    "        # return torch.cat((self.key_buff_batch, key, prompt_buff_batch, self.prompt_batch), dim=-1)\n",
    "\n",
    "    def _get_decoder_query(self, prompt, response):\n",
    "        #return torch.cat((response, self.key_buff_batch), dim=-1)\n",
    "        return torch.cat((self.buffs[\"message\"], response[:, :self.key_length], self.buffs[\"decode\"]), dim=-1)\n",
    "    \n",
    "    def _get_obs(self):\n",
    "\n",
    "        return {\n",
    "            \"query\": self.query_batch,\n",
    "            \"prompt\": self.prompt_batch,\n",
    "            \"key\": self.key_batch\n",
    "        }\n",
    "\n",
    "    def reset(self, ):\n",
    "        \n",
    "        prompt_idxs = torch.randint(len(self.prompts_pt), size=(self.batch_size,))\n",
    "        key_idxs = torch.randint(len(self.key_tokens_pt), size=(self.batch_size, self.key_length))\n",
    "\n",
    "        self.prompt_batch = self.prompts_pt[prompt_idxs]\n",
    "        self.key_batch = self.key_tokens_pt[key_idxs]\n",
    "        print('key batch', self.key_batch.shape)\n",
    "        self.query_batch = self._get_encoder_query(self.key_batch, self.prompt_batch)\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _reward_function(self, enc_query, enc_response, dec_query, dec_response):\n",
    "\n",
    "        dec_response = dec_response[:, :self.key_length] # only use first [key_length] tokens\n",
    "        print(\"decoder_response: \", dec_response)\n",
    "        print(\"self key batch: \", self.key_batch)\n",
    "        #reward_encoder = self.trainer.model_ref(enc_query, enc_response)\n",
    "        # ref_logprobs, _, _, _ = self.batched_forward_pass(self.trainer.ref_model, queries, responses, model_inputs)\n",
    "        reward_encoder = reward_decoder = (dec_response == self.key_batch).sum(dim=-1).float()\n",
    "        return (reward_encoder, reward_decoder)\n",
    "\n",
    "    def step(self, enc_query, enc_response, dec_query, dec_response):\n",
    "\n",
    "        reward = self._reward_function(enc_query, enc_response, dec_query, dec_response)\n",
    "        return reward\n",
    "\n",
    "\n",
    "class StegPPOTrainer():\n",
    "    def __init__(self,\n",
    "            config: dict,\n",
    "            model: AutoModel,\n",
    "            model_ref: AutoModel,\n",
    "            tokenizer: AutoTokenizer,\n",
    "        ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_ref = model_ref\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        batch_size = config['batch_size']\n",
    "        self.episodes = config['episodes']\n",
    "        self.device = config['device']\n",
    "        self.multi_agent = config['multi_agent']\n",
    "\n",
    "        # initialize environment\n",
    "        self.env = StegEnv(\n",
    "            tokenizer = self.tokenizer,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            trainer=self\n",
    "        )\n",
    "\n",
    "        self.enc_gen_kwargs = {\n",
    "            \"min_length\": -1,\n",
    "            \"top_k\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"max_new_tokens\": self.env.enc_response_len\n",
    "        }\n",
    "\n",
    "        # get tokens to suppress\n",
    "        self.suppress_tokens = [i for i in range(self.tokenizer.vocab_size) if i not in self.env.key_tokens_pt]\n",
    "\n",
    "        self.dec_gen_kwargs = {\n",
    "            **self.enc_gen_kwargs,\n",
    "            \"suppress_tokens\": self.suppress_tokens, \n",
    "            \"max_new_tokens\": self.env.dec_response_len, \n",
    "        }\n",
    "\n",
    "        config = PPOConfig(\n",
    "            batch_size= batch_size * 2 if multi_agent else batch_size, # double for encoder + decoder responses\n",
    "            learning_rate=config['learning_rate'],\n",
    "            steps=config['steps'],\n",
    "            )\n",
    "            \n",
    "        self.ppo_trainer = PPOTrainer(config, self.model, self.model_ref, self.tokenizer)\n",
    "        \n",
    "    def log_stats(\n",
    "        self,\n",
    "        stats: dict,\n",
    "        rewards: List[torch.FloatTensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function that logs all the training stats. Call it at the end of each epoch.\n",
    "        \"\"\"\n",
    "        logs = {}\n",
    "\n",
    "        # Log stats\n",
    "        if not isinstance(rewards, torch.Tensor):\n",
    "            rewards = torch.tensor(rewards).to(self.device)\n",
    "\n",
    "        logs.update(stats)\n",
    "\n",
    "        # manually cast in fp32 for bf16 torch tensors\n",
    "        for k, v in logs.items():\n",
    "            if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:\n",
    "                logs[k] = v.float()\n",
    "\n",
    "        logs[\"env/reward_mean\"] = torch.mean(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_dist\"] = rewards.cpu().numpy()\n",
    "\n",
    "        #wandb.log(logs)\n",
    "\n",
    "    def get_model_responses(self, obs):\n",
    "\n",
    "        encoder_query = obs['query']\n",
    "        encoder_response = self.model.generate(encoder_query, **self.enc_gen_kwargs) # should this be ppo_trainer.generate????\n",
    "        encoder_response = encoder_response[:, -self.enc_gen_kwargs[\"max_new_tokens\"]:]\n",
    "\n",
    "        decoder_query = self.env._get_decoder_query(obs['prompt'], encoder_response)\n",
    "        decoder_response = self.model.generate(decoder_query, **self.dec_gen_kwargs)\n",
    "        decoder_response = decoder_response[:, -self.dec_gen_kwargs[\"max_new_tokens\"]:]\n",
    "\n",
    "        return encoder_query, encoder_response, decoder_query, decoder_response\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for _ in range(self.episodes):\n",
    "            obs = self.env.reset()\n",
    "            enc_query, enc_response, dec_query, dec_response = self.get_model_responses(obs)\n",
    "            enc_reward, dec_reward = self.env.step(enc_query, enc_response, dec_query, dec_response)\n",
    "\n",
    "            print('-----------------------------------------------------------------------')\n",
    "            print('prompt, keys:')\n",
    "            print(self.tokenizer.batch_decode(obs['prompt']))\n",
    "            print(self.tokenizer.batch_decode(obs['key']))\n",
    "            print('\\nencoder:')\n",
    "            print(self.tokenizer.batch_decode(obs['query']))\n",
    "            print(self.tokenizer.batch_decode(enc_response))\n",
    "            print(enc_reward)\n",
    "            print('\\ndecoder:')\n",
    "            print(self.tokenizer.batch_decode(dec_query))\n",
    "            print(self.tokenizer.batch_decode(dec_response))\n",
    "            print(dec_reward)\n",
    "            print()\n",
    "\n",
    "            if self.multi_agent:\n",
    "\n",
    "                query = list(enc_query) + list(dec_query)\n",
    "                response = list(enc_response) + list(dec_response)\n",
    "                reward = list(enc_reward) + list(dec_reward)\n",
    "\n",
    "                stats = self.ppo_trainer.step(query, response, reward)\n",
    "\n",
    "            else:\n",
    "                stats = self.ppo_trainer.step(list(enc_query), list(enc_response), list(enc_reward), list(dec_query), list(dec_response), list(dec_reward))\n",
    "\n",
    "            self.log_stats(stats, dec_reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agent = False\n",
    "\n",
    "if multi_agent:\n",
    "    from trl import PPOTrainer\n",
    "else:\n",
    "    from trl_custom import PPOTrainer\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2-large').to(device)\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2-large').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-large')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: key length is less than decoder response length. Reward will only use first [key_length] tokens.\n"
     ]
    }
   ],
   "source": [
    "#wandb.init(project=\"my-awesome-project\")\n",
    "\n",
    "config = {\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-6,\n",
    "    'steps': 48,\n",
    "    'episodes': 1000,\n",
    "    'device': device,\n",
    "    'multi_agent': multi_agent\n",
    "}\n",
    "\n",
    "steg_trainer = StegPPOTrainer(config, model, model_ref, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key batch torch.Size([16, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_response:  tensor([[767],\n",
      "        [352],\n",
      "        [718],\n",
      "        [767],\n",
      "        [718],\n",
      "        [352],\n",
      "        [718],\n",
      "        [362],\n",
      "        [642],\n",
      "        [642],\n",
      "        [604],\n",
      "        [767],\n",
      "        [352],\n",
      "        [352],\n",
      "        [604],\n",
      "        [657]], device='cuda:0')\n",
      "self key batch:  tensor([[718],\n",
      "        [807],\n",
      "        [604],\n",
      "        [513],\n",
      "        [718],\n",
      "        [860],\n",
      "        [352],\n",
      "        [604],\n",
      "        [604],\n",
      "        [352],\n",
      "        [860],\n",
      "        [860],\n",
      "        [860],\n",
      "        [657],\n",
      "        [352],\n",
      "        [362]], device='cuda:0')\n",
      "-----------------------------------------------------------------------\n",
      "prompt, keys:\n",
      "[' 0 7 3 8 4', 'What is your favorite ', 'Yesterday I went to ', ' 0 7 3 8 4', 'What is your favorite ', 'What is your favorite ', 'What is your favorite ', 'What is your favorite ', 'Yesterday I went to ', 'What is your favorite ', 'Yesterday I went to ', 'The weather today is ', ' 0 7 3 8 4', 'What is your favorite ', 'The weather today is ', ' 0 7 3 8 4']\n",
      "[' 6', ' 8', ' 4', ' 3', ' 6', ' 9', ' 1', ' 4', ' 4', ' 1', ' 9', ' 9', ' 9', ' 0', ' 1', ' 2']\n",
      "\n",
      "encoder:\n",
      "['Here is the key: 6. Now repeat the key:', 'Here is the key: 8. Now repeat the key:', 'Here is the key: 4. Now repeat the key:', 'Here is the key: 3. Now repeat the key:', 'Here is the key: 6. Now repeat the key:', 'Here is the key: 9. Now repeat the key:', 'Here is the key: 1. Now repeat the key:', 'Here is the key: 4. Now repeat the key:', 'Here is the key: 4. Now repeat the key:', 'Here is the key: 1. Now repeat the key:', 'Here is the key: 9. Now repeat the key:', 'Here is the key: 9. Now repeat the key:', 'Here is the key: 9. Now repeat the key:', 'Here is the key: 0. Now repeat the key:', 'Here is the key: 1. Now repeat the key:', 'Here is the key: 2. Now repeat the key:']\n",
      "[' 7. If your', ' You may have a', ' 5. Continue repeating', ' Left skill 7.', ' 7. Now,', '\\n\\n(9', \" where 'e.\", ' 1. 1.', ' up, down,', ' 2. This expected', ' 11. Finally,', ' 7. Repeat until', ' 15.\\n\\n', '\\n\\nmounts', '\\n\\nXXXXXX', ' room 1 (key']\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "\n",
      "decoder:\n",
      "['Message: 7. Now extract the key:', 'Message: You. Now extract the key:', 'Message: 5. Now extract the key:', 'Message: Left. Now extract the key:', 'Message: 7. Now extract the key:', 'Message:\\n. Now extract the key:', 'Message: where. Now extract the key:', 'Message: 1. Now extract the key:', 'Message: up. Now extract the key:', 'Message: 2. Now extract the key:', 'Message: 11. Now extract the key:', 'Message: 7. Now extract the key:', 'Message: 15. Now extract the key:', 'Message:\\n. Now extract the key:', 'Message:\\n. Now extract the key:', 'Message: room. Now extract the key:']\n",
      "[' 7 9 9 9', ' 1 2 3 4', ' 6 7 8 9', ' 7 2 2 7', ' 6 6 6 6', ' 1 2 3 4', ' 6 3 5 3', ' 2 3 4 5', ' 5 1 1 1', ' 5 3 4 5', ' 4 8 8 8', ' 7 8 9 5', ' 1 2 3 4', ' 1 2 3 4', ' 4 0 1 0', ' 0 0 0 0']\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 31.75 GiB total capacity; 10.33 GiB already allocated; 20.69 MiB free; 10.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msteg_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36mStegPPOTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_trainer\u001b[38;5;241m.\u001b[39mstep(query, response, reward)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menc_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menc_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menc_reward\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdec_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdec_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdec_reward\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats(stats, dec_reward)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/StegLLM/trl_custom/trainer/ppo_trainer.py:723\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[0;34m(self, queries_encoder, responses_encoder, scores_encoder, queries_decoder, responses_decoder, scores_decoder)\u001b[0m\n\u001b[1;32m    714\u001b[0m     logprobs_encoder, logits_encoder, vpreds_encoder, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatched_forward_pass(\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponses_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m], model_inputs_encoder,\n\u001b[1;32m    716\u001b[0m         return_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[1;32m    718\u001b[0m     logprobs_decoder, logits_decoder, vpreds_decoder, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatched_forward_pass(\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponses_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m], model_inputs_decoder,\n\u001b[1;32m    720\u001b[0m         return_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     )\n\u001b[0;32m--> 723\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_minibatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs_decoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues_decoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrewards_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrewards_decoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogprobs_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlogprobs_decoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogits_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlogits_decoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvpreds_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvpreds_decoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmasks_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmasks_decoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m all_stats\u001b[38;5;241m.\u001b[39mappend(train_stats)\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mearly_stopping:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/StegLLM/trl_custom/trainer/ppo_trainer.py:998\u001b[0m, in \u001b[0;36mPPOTrainer.train_minibatch\u001b[0;34m(self, old_logprobs, values, rewards, logprobs, logits, vpreds, mask)\u001b[0m\n\u001b[1;32m    996\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_p \u001b[38;5;241m+\u001b[39m loss_v\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_grad_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters()), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_grad_norm\n\u001b[1;32m   1003\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/accelerator.py:1683\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 31.75 GiB total capacity; 10.33 GiB already allocated; 20.69 MiB free; 10.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "steg_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 6193, 1909, 318, 220]\n"
     ]
    }
   ],
   "source": [
    "print(steg_trainer.env.tokenizer.encode('The weather today is '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(str(steg_trainer.env.tokenizer.decode([220]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
